\documentclass[a4paper]{article}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{rotating}
\usepackage[pdftex,bookmarks=TRUE]{hyperref}
\usepackage{float}
\usepackage{rotating}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{theorem}
\usepackage{calc}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{bm}
\usepackage[cm]{fullpage}
\usepackage{dcolumn}
% General parameters, for ALL pages:
\renewcommand{\topfraction}{0.99}    %
\renewcommand{\bottomfraction}{0.99}           % max fraction of floats at bottom
% Parameters for TEXT pages (not float pages):
\setcounter{topnumber}{55}
\setcounter{bottomnumber}{55}
\setcounter{totalnumber}{55} % 2 may work better
\setcounter{dbltopnumber}{55} % for 2-column pages
\renewcommand{\dbltopfraction}{0.99}             % fit big float above 2-col. text
\renewcommand{\textfraction}{0.01}  % allow minimal text w. figs
% Parameters for FLOAT pages (not text pages):
\renewcommand{\floatpagefraction}{0.9}          % require fuller float pages
                % N.B.: floatpagefraction MUST be less than topfraction !!
\renewcommand{\dblfloatpagefraction}{0.9}   % require fuller float pages

\begin{document}
\title{Dimensionality Reduction methods}
\maketitle

In this paper some dimensionality reduction techniques are
examined. The problem could be formulated as follows: transform a
high-dimensional data set into the lower meaningful representation of
this data set so that the latter captures the content of the
former one as much as possible. In mathematical terms, suppose that
we have the original data set $X$ of dimension $n \times D$, $n$ -
samples, $D$ - features. The problem is to transform $X$ into $Y$ of
dimension $n \times d$, where $d < D$. d is the intrinsic dimension of
the data set $X$ - the minimum number of features that describes
original data set.

There are lots of methods for dimensionality reduction. As a reference
this paper follows Matlab Toolbox for Dimensionality Reduction \footnote{"An
  Introduction to Dimensionality Reduction Using Matlab.pdf"} and
considers ones which are implemented in the R package. To illustrate techniques the Gross
Value Added(GVA) data set is used. The main aim is to get the
representation of the original data set so that the visualization
would be possible ($d = 2$). The value
added is expressed per 1000 of households and measured in US\$ millions\footnote{Constant
  prices, fixed exchange rate(2012)}. GVA is divided into $D=7$
categories:
\begin{itemize}
 \item GVA from Agriculture, Hunting, Forestry and Fishing;
 \item GVA from Manufacturing, Mining, Gas and Water Supply;
 \item GVA from Construction;
 \item GVA from Transport, Communications, Trade, Hotels and Restaurants;
 \item GVA from Financial Intermediation, Real Estate, Renting and Business Activities;
 \item GVA from Education, Health, Social Services, Public and Undefined Sectors;
 \item GVA from Activities of Households;
\end{itemize}
The data frame contains some NA entries so the \textit{na.omit()}
function was used. The code for transforming the data:

<<echo=FALSE>>=
setwd("../R/code/")
@
<<echo=TRUE>>=
library(ggplot2)
library(grid)
library(RColorBrewer)
source("01readata.R")
data <- gva.con.f.hh
data <- melt(data[, -3], id = c("Region", "Country", "Subcategory"))
colnames(data)[4] <- "Year"
data <- cast(data, Region + Country + Year ~ Subcategory)
data.omit <- na.omit(data)
data.omit$Country <- factor(as.character(data.omit$Country))
data.omit$Region <- factor(as.character(data.omit$Region))
@

\section{Traditional linear techniques}
In this section two linear dimensionality reduction techniques will be
considered: a principal components analysis (PCA) and a linear discriminant
analysis. These techniques result in each of the $d < D$ components of the new variable being a linear combination of the original variables:

\begin{align}
  &\mathrm{y}_i = w_{i,1}\mathrm{x}_1 + \ldots + w_{i,D}\mathrm{x}_D,\hspace{0.25cm} i =
  1,2,\ldots, d, \hspace{0.25cm} \text{or}\\
  &\mathrm{Y} = \mathrm{W}\mathrm{X} \label{eq:trans}, \hspace{0.25cm} \mathrm{W} = d \times D \hspace{0.25cm} \text{matrix}.
\end{align}

\subsection{Principal component analysis}
\subsubsection{Method}
PCA seeks to reduce the dimension of the data by finding a few
orthogonal linear combinations (the PCs) of the original variables with the largest variance. The first PC, $\mathrm{y}_1$, is the linear combination with
the largest variance. We have $\mathrm{y}_1 = \mathrm{X}^T\mathrm{w}_1$, where the D-dimensional
coefficient vector $\mathrm{w}_1 = (w_{1,1},\ldots,w_{1,D})^T$ solves
\begin{align*}
 \mathrm{w}_1 = \arg\max_{||\mathrm{w}=1||}\mathrm{Var}(\mathrm{X}^T\mathrm{w})
\end{align*}
The second PC is the linear combination with the second largest
variance and orthogonal to the first PC, and so on. There are as many
PCs as the number of the original variables. For many datasets, the
first several PCs explain most of the variance, so that the rest can
be disregarded with minimal loss of information. Since the variance depends on the scale of the variables, it is customary to first standardize each variable
to have mean zero and standard deviation one. After the standardization, the original variables with possibly
different units of measurement are all in comparable units. It can be
showed that a matrix $\mathrm{W}$ could be given by $D \times D$ matrix $\mathrm{U}^T$
from a spectral
decomposition of the covariance matrix of the standardized data:
\begin{align*}
\Sigma &= \mathrm{U} \Lambda \mathrm{U}^T, \hspace{0.25cm} \text{where}\\
 \Sigma &=\frac{1}{n}\mathrm{X}\mathrm{X}^T
\end{align*}

\textbf{Strong points}:
\begin{itemize}
\item The best, in mean-square error sense, linear dimension reduction
  technique.
\end{itemize}

\textbf{Weak points}:
\begin{itemize}
\item PCs do not necessarily correspond to meaningful physical
  quantities;
\item It is not obvious how many PCs to keep;
\item It might be infeasible to carry out the decomposition of the data
  covariance matrix for very high-dimensional data sets;
\item It is only able to find the linear subspace and thus cannot deal
  properly with data lying on nonlinear manifolds.
\end{itemize}
\subsubsection{R}
The \textit{prcomp()} function from the \textit{stats} library is used
for a principal component analysis.

<<echo=TRUE>>=
library(stats)
res.pca <- prcomp(data.omit[, -c(1:3)], scale = TRUE)
names(res.pca)
@
The projection matrix $W$ is under the label "rotation".
<<echo=TRUE>>=
summary(res.pca)
@
As mentioned before the main aim is a visualization, so two PC's
are kept. The summary of the object \textit{res.pca} shows that the
first $2$ PCs explain about 77\% of the variation. The following code
generates the graph.

<<echo=TRUE>>=
Y.pca <- cbind(data.omit[, c("Country", "Year")], res.pca[["x"]][, c(1, 2)])
colnames(Y.pca)[c(3, 4)] <- c("Comp1", "Comp2")
q.pca <- myplot(data = Y.pca)
@

<<echo=FALSE, results = hide>>=
pdf("../../documents/graphs/pca.pdf", height = 8, width = 17)
print(q.pca)
dev.off()
@

\begin{figure}[!htbp]
\begin{center}
\includegraphics{graphs/pca.pdf}
\caption{Principal Components Analysis.}
\end{center}
\end{figure}

\subsection{Linear Discriminant analysis}
\subsubsection{Method}
PCA is unsupervised in that it does not take class labels into
account. By contrast Linear Discriminant Analysis (LDA)
discovers projections on which class differences are taken into
account. The objective is to uncover a transformation that will
maximise between-class separation and minimise within-class
separation. To do this two scatter matrices are defined, SB for
between-class separation and SW for within-class separation:

\begin{align*}
  S_B &= \sum_{c \in Class}n_c(\mu_c - \mu)(\mu_c - \mu)^T\\
  S_W &= \sum_{c \in Class}\sum_{j:x_j \in c}(x_j - \mu_c)(x_j - \mu_c)^T,
\end{align*}
where $n_c$ is the number of samples in class c, $\mu_c$ - the mean in
class c, $\mu$ - the mean of all samples:
\begin{align*}
 \mu = \frac{1}{n}\sum_{i=1}^nx_i \hspace{0.25cm}
 \mu_c=\frac{1}{n_c}\sum_{j:x_j \in c}x_j
\end{align*}
The components within these summations $(\mu, \mu_c, x_j)$ are vectors
of dimension D. The objectives of maximising between-class separation
and minimising within-class separation can be combined into a single maximisation called the Fisher criterion:
\begin{align*}
  W_{LDA} = \arg \max_W \frac{|W^TS_BW|}{|W^TS_WW|}
\end{align*}
This matrix $W_{LDA}$ provides the transformation described in
equation (\ref{eq:trans}).

\textbf{Strong points}:
\begin{itemize}
 \item It is supervised learning method, so, where appropriate, it can take class differences into account.
\end{itemize}

\textbf{Weak points}:
\begin{itemize}
 \item As with PCA the constraint that the transformation is linear is
   sometimes considered restricting;
 \item Some algorithms require that $S_W$ be non-singular. In case of
   the high-dimensional data set, it could be a problem.
 %\item It is not so obvious how to choose the dimension of the $W_{LDA}$ matrix.
\end{itemize}

\subsubsection{R}
The \textit{MASS} library
contains the function
\textit{lda()} for carrying out
a Linear Discriminant analysis. In case of our data set countries or
regions could be considered as classes. In this example regions are
taken as classes. There are $6$
regions or classes and the \textit{lda()} function returns a scaling matrix
that has the dimension $D \times (|C|-1)$, where |C| is the number of classes.
In order to get a visual representation of the data set a division
into $3$ classes are required. Let's take the subset of our data set
which has "Eastern Europe", "Western Europe" and "North America" regions.

<<echo=TRUE>>=
library(MASS)
data.lda <- data.omit[data.omit$Region %in%
                      c("Eastern Europe", "Western Europe", "North America"), ]
data.lda$Region <- factor(as.character(data.lda$Region))
res.lda <- lda(Region ~., data.lda[, -c(2, 3)])
Y.lda <- cbind(data.lda[, c("Country", "Year")],
               as.matrix(data.lda[, -c(1:3)]) %*% res.lda$scaling)
colnames(Y.lda)[c(3, 4)] <- c("Comp1", "Comp2")
q.lda <- myplot(data = Y.lda)
@

The object res.lda\$scaling stands for the $W_{LDA}$ matrix which, in our
case, is of dimension $ 7 \times 2$.

<<echo=FALSE, results = hide>>=
pdf("../../documents/graphs/lda.pdf", height = 8, width = 17)
print(q.lda)
dev.off()
@

\begin{figure}[!htbp]
\begin{center}
\includegraphics{graphs/lda.pdf}
\caption{Linear Discriminant Analysis.}
\end{center}
\end{figure}


\section{Global nonlinear techniques}
Global nonlinear techniques for dimensionality reduction are
techniques that attempt to preserve global properties of the data, but that are capable of constructing nonlinear transformations between the
high-dimensional data representation X and its low-dimensional
counterpart Y . The subsection presents five global nonlinear
techniques for dimensionality reduction: Multidimensional scaling(MDS),
Kernel PCA, Isomap, t-Distributed Stochastic neighbor embedding(t-SNE) and Stochastic proximity embedding(SPE).

\subsection{Multidimensional scaling}
\subsubsection{Method}
Given n items in a D-dimensional space and a $n \times n$ matrix of
proximity measures among the items, multidimensional scaling (MDS) produces a d-dimensional representation of the items such that the
distances among the points in the new space reflect the proximities in
the data. The proximity measures the (dis)similarities among the items, and in general, it is a distance measure: the more similar two
items are, the smaller their distance is.

Given the n D-dimensional items, $\{x_i\}_{i=1}^n$, and a symmetric
distance matrix $\Delta = \{\delta_{ij}, i,j = 1,\ldots,n\}$, the result of a d-dimensional MDS
will be the d-dimensional items, $\{y_i\}_{i=1}^n$, such that the
distances $d_{ij}=d(y_i,y_j)$ are as close as possible to a function
$f$ of the corresponding proximities $f(\delta_{ij})$. MDS methods that incorporate the given distances $\delta_{ij}$ into their calculations are called metric
methods, while the ones that only use the rank ordering of the
distances are called non-metric methods. In contrast, depending on whether $f$
is linear or non-linear, MDS is called either metric or non-metric, correspondingly.

The steps for the most general estimation procedure are as follows:

\begin{enumerate}
  \item Define the stress function as an objective function to be
    minimized by $f$:
    \begin{align*}
     \text{stress}_f(f, \Delta,
     X)=\sqrt{\frac{\sum_{i,j}[f(\delta_{ij})-d_{ij}]^2}{\text{scale
           factor}}},
    \end{align*}
    where the scale factor is usally based on
    $\sum_{ij}[f(\delta_{ij})]^2$ or on $\sum_{ij}d_{ij}^2$
  \item given X find $\hat{f}$ that minimizes a stress function:
    \begin{align*}
     \hat{f}= \arg\min_f[\text{stress}_f(f, \Delta, X)],
    \end{align*}
  \item determine the optimal $\hat{X}$ by:
    \begin{align*}
     \hat{X}= \arg\min_X[\text{stress}(\hat{f}, \Delta, X)].
    \end{align*}
\end{enumerate}

\textbf{Strong points}:
\begin{itemize}
 \item MDS methods are capable of constructing nonlinear
   transformations between the high-dimensional data representation X and its low-dimensional counterpart Y.
\end{itemize}

\textbf{Weak points}:
\begin{itemize}
\item It is difficult to select the appropriate dimension of Y;
\item MDS does a much better job in representing large distances (the
  global structure) than small ones (the local structure);
\item Contrarily to principal component analysis, in MDS one cannot
  obtain an (d-1)-dimensional map out of an d-dimensional one by dropping one coordinate (or, in general, by linearly projecting along
some direction).
\end{itemize}

\subsubsection{R}
In this paper two functions for a Multidimensional Scaling are used. The
\textit{cmdscale()} function for classical(metric) MDS and the \textit{isoMDS()} - for
non-metric MDS. In the first case the stress is of the form:
\begin{align*}
     \sqrt{\frac{\sum_{i,j}[\delta_{ij}-d_{ij}]^2}{\sum_{i,j}d_{i,j}^2}}.
\end{align*}
In the second case the method is called non-metric in the sense that
the procedure uses a rank ordering of distances. The \textit{isoMDS()}
function uses the Kruskal's stress function:
\begin{align*}
     \sqrt{\frac{\sum_{i,j}[\hat{d}_{ij}-d_{ij}]^2}{\sum_{i,j}d_{i,j}^2}},
\end{align*}
where $\hat{d}_{i,j}$ is fitted distances such that having the same rank
order as $\delta_{i,j}$.

<<echo=TRUE>>=
dd <- dist(data.omit[, -c(1:3)], method = "euclidean")
res.mds1 <- cmdscale(dd, k = 2)
Y.mds1 <- cbind(data.omit[, c("Country", "Year")], res.mds1)
colnames(Y.mds1)[c(3, 4)] <- c("Comp1", "Comp2")
q.mds1 <- myplot(data = Y.mds1)
@

The object \textit{res.mds1} is the low-dimensional representation of
the original data matrix.

<<echo=FALSE, results = hide>>=
pdf("../../documents/graphs/mds1.pdf", height = 8, width = 17)
print(q.mds1)
dev.off()
@

\begin{figure}[!htbp]
\begin{center}
\includegraphics{graphs/mds1.pdf}
\caption{Classical Multidimensional Scaling.}
\end{center}
\end{figure}

<<echo=TRUE>>=
res.mds2 <- isoMDS(dd, k = 2)
Y.mds2 <- cbind(data.omit[, c("Country", "Year")], res.mds2$points)
colnames(Y.mds2)[c(3, 4)] <- c("Comp1", "Comp2")
q.mds2 <- myplot(data = Y.mds2)
@
The object \textit{res.mds2\$points} stands for the $Y$ matrix.

<<echo=FALSE, results = hide>>=
pdf("../../documents/graphs/mds2.pdf", height = 8, width = 17)
print(q.mds2)
dev.off()
@

\begin{figure}[!htbp]
\begin{center}
\includegraphics{graphs/mds2.pdf}
\caption{Non-metric Multidimensional Scaling.}
\end{center}
\end{figure}

\subsection{Kernel PCA}
\subsubsection{Method}
Kernel PCA (KPCA) is the reformulation of traditional linear PCA in a
high-dimensional space that is constructed using a kernel
function. Kernel PCA computes the principal eigenvectors of the kernel
matrix, rather than those of the covariance matrix. The reformulation
of traditional PCA in kernel space is straightforward, since a kernel
matrix is similar to the inproduct of the datapoints in the high-dimensional space that is constructed using the kernel
function. Kernel PCA computes the kernel matrix K of the datapoints $x_i$. The entries in the kernel matrix are defined by:
\begin{align*}
    k_{ij}= \kappa(x_i, x_j),
\end{align*}
where $\kappa$ is a kernel function. Subsequently, the kernel matrix K
is centered using the following modification of the entries:
\begin{align*}
  k_{ij}= k_{ij}-\frac{1}{n}\sum_l k_{il}-\frac{1}{n}\sum_l k_{jl} + \frac{1}{n^2}\sum_{lm} k_{lm}
\end{align*}
The centering operation corresponds to subtracting the mean of the
features in traditional PCA. It makes sure that the features in the
high-dimensional space defined by the kernel function are
zero-mean. Subsequently, the principal d eigenvectors $v_i$ of the centered kernel matrix are computed. It can be shown that the eigenvectors of the covariance
matrix $\alpha_i$(in the high-dimensional space constructed by
$\kappa$) are scaled versions of the eigenvectors of the kernel matrix
$v_i$:
\begin{align*}
  \alpha_i = \frac{1}{\sqrt{\lambda_i}}v_i
\end{align*}
In order to obtain the low-dimensional data representation, the data
is projected onto the eigenvectors of the covariance matrix. The result of the projection (i.e., the low-dimensional data representation Y ) is given by:
\begin{align*}
  Y = \Big\{\sum_j\alpha_1\kappa(x_j,x),\sum_j\alpha_2\kappa(x_j,x),\ldots,\sum_j\alpha_d\kappa(x_j,x)\Big\}
\end{align*}

\textbf{Strong points}:
\begin{itemize}
  \item Capability of constructing a nonlinear mapping with little computational cost.
  \item The computational complexity of KPCA does not grow with the
    dimensionality of the feature space.
  \item No nonlinear optimization is involved - what is only needed is
    to solve an eigenvalue problem as in the case of standard PCA.
\end{itemize}

\textbf{Weak points}:
\begin{itemize}
  \item The mapping highly relies on the choice of the kernel function $\kappa$.
\end{itemize}

\subsubsection{R}
The \textit{kernlab} package has the function \textit{kpca()} for
carrying out a kernal PCA dimensionality reduction method.

<<echo=TRUE>>=
library(kernlab)
kpc <- kpca(~., data = data.omit[, -c(1:3)], kernel = "rbfdot", features = 2)
Y <- rotated(kpc)
Y.kpca <- cbind(data.omit[, c("Country", "Year")], Y)
colnames(Y.kpca)[c(3, 4)] <- c("Comp1", "Comp2")
q.kpca <- myplot(data = Y.kpca)
@

The \textit{rotated()} function returns the original data projected
(rotated) on the principal components.

<<echo=FALSE, results = hide>>=
pdf("../../documents/graphs/kpca.pdf", height = 8, width = 17)
print(q.kpca)
dev.off()
@

\begin{figure}[!htbp]
\begin{center}
\includegraphics{graphs/kpca.pdf}
\caption{Kernel Principal Components Analysis.}
\end{center}
\end{figure}


\subsection{t-Distributed Stochastic Neighbor Embedding}
In this section two dimensionality reduction techniques are examined:
Stochastic Neighbor Embedding(SNE) and its extension - t-Distributed
Stochastic Neighbor Embedding(t-SNE).
\subsubsection{Methods}
Stochastic Neighbor Embedding is an iterative technique that (similar to MDS) attempts to retain the pairwise
distances between the datapoints in the low-dimensional representation
of the data. SNE differs from MDS by the distance measure that is used, and by the cost function that it minimizes. In SNE, similarities of nearby points
contribute more to the cost function. This leads to a low-dimensional
data representation that preserves mainly local properties of the manifold.

The similarity of datapoint $x_j$ to datapoint $x_i$ is the
probability, $p_{ij}$, that $x_i$ would pick $x_j$ as its neighbor
if neighbors were picked in proportion to their probability density
under a Gaussian centered at $x_i$. For nearby datapoints, $p_{ij}$ is relatively high, whereas for widely separated datapoints, $p_{ij}$ will be
almost infinitesimal. Mathematically, the probability $p_{ij}$ is given by:
\begin{align*}
p_{ij}=\frac{\exp\{-||x_i-x_j||^2/2\sigma^2\}}{\sum_{k \neq l}\exp\{-||x_k-x_l||^2/2\sigma^2\}}.
\end{align*}
For the low-dimensional counterparts $y_i$ and $y_j$ of the
high-dimensional datapoints $x_i$ and $x_j$, it is possible to compute
a similar conditional probability, $q_{ij}$.
In a perfect low-dimensional representation of the data, $p_{ij}$ and
$q_{ij}$ are equal. Hence, SNE minimizes the mismatch between
$p_{ij}$ and $q_{ij}$. Kullback-Leibler divergences are a natural
distance measure to measure the difference between two probability distributions. SNE attempts to minimize the following sum of Kullback-Leibler divergences:

\begin{align*}
 C=\sum_{ij}p_{ij}\log\frac{p_{ij}}{q_{ij}}.
\end{align*}
The above method suffers from the "crowding problem": when the intrinsic dimensionality of the data exceeds the embedding dimensionality, there is
not enough space in the embedding to allow data points to separate.
Therefore, data points are forced to collapse on top of each other
in the embedding. t-SNE uses a Student-t distribution rather than a Gaussian to
compute the similarity between two points in the low-dimensional
space. t-SNE employs a heavy-tailed distribution in the low-dimensional space to alleviate the crowding problem.

\textbf{Strong points}:
\begin{itemize}
  \item The use of heavy tailed distribution avoids the "crowding problem".
\end{itemize}

\textbf{Weak points}:
\begin{itemize}
  \item It's typically only used as a visualization method (d = 2 or
    3).
  \item It reduces the dimensionality of data mainly based on
local properties of the data, which makes t-SNE sensitive to the curse
of the intrinsic dimensionality of the data. As a result, t-SNE might be less successful if it is applied on data sets with a very high intrinsic dimensionality.
  \item t-SNE is not guaranteed to converge to a global optimum of its
    cost function.
\end{itemize}

\subsubsection{R}

The \textit{R} package has the \textit{tsne} library for a
t-distributed Stochastic Neighbor Embedding multidimensional scaling method.

<<echo=TRUE>>=
library(tsne)
res.tsne <- tsne(data.omit[, -c(1:3)], k = 2, max_iter = 2000)
Y.tsne <- cbind(data.omit[, c("Country", "Year")], res.tsne)
colnames(Y.tsne)[c(3, 4)] <- c("Comp1", "Comp2")
q.tsne <- myplot(data = Y.tsne)
@

<<echo=FALSE, results = hide>>=
pdf("../../documents/graphs/tsne.pdf", height = 8, width = 17)
print(q.tsne)
dev.off()
@

\begin{figure}[!htbp]
\begin{center}
\includegraphics{graphs/tsne.pdf}
\caption{t-Stochastic Neighbor Embedding.}
\end{center}
\end{figure}

\newpage
\subsection{Stochastic Proximity Embedding}
\subsubsection{Method}
Stochastic Proximity Embedding (SPE) is an iterative algorithm that
minimizes the MDS raw stress function. SPE differs from MDS in the
efficient rule it employs to update the current estimate of the
low-dimensional data representation. SPE minimizes the MDS raw stress function:

\begin{align*}
  S=\sum_{ij}(d_{ij}-r_{ij})^2,
\end{align*}
where $r_{ij}$ is the proximity between the high-dimensional
datapoints $x_i$ and $x_j$, and $d_{ij}$ is the Euclidean distance between their low-dimensional counterparts $y_i$ and $y_j$ in the current approximation of the
embedded space. The form of the stress function $S$ could be
different, e.g.  Kruskal's or Sammon's. SPE performs an iterative algorithm in order to minimize the raw stress function defined above. The initial positions
of the points $y_i$ are selected randomly. An update of the embedding coordinates $y_i$ is performed by randomly
selecting s pairs of points ($y_i$, $y_j$). For each pair of points, the Euclidean distance in the low-dimensional data representation
$Y$ is computed. Subsequently, the coordinates of $y_i$ and $y_j$ are updated in order to decrease the difference
between the distance in the original space $r_{ij}$ and the distance
in the embedded space $d_{ij}$. The updating is performed using the following update rules:

\begin{align*}
  y_i &= y_i + \lambda \frac{r_{ij}-d_{ij}}{2d_{ij} + \epsilon}(y_i - y_j)\\
  y_j &= y_j + \lambda \frac{r_{ij}-d_{ij}}{2d_{ij} + \epsilon}(y_j - y_i),
\end{align*}
where $\lambda$ is a learning parameter that decreases with the number of iterations, and $\epsilon$ is a regularization parameter that
prevents divisions by zero.

\textbf{Strong points}:
\begin{itemize}
  \item Fast. It is known that the relative configuration of $N$ points in a
    $D$-dimensional space can be fully described using only $(N - D/2
    - 1)/(D - 1)$ distances. SPE exploits this redundancy through random sampling.
\end{itemize}

\textbf{Weak points}:
\begin{itemize}
  \item The performance of the algorithm depends on the parameter
    $\lambda$. If $\lambda$ is too small, self-organization will be
    smooth, but will converge slowly. If $\lambda$ is too large,
    convergence will be fast but the map may become unstable.
\end{itemize}

\subsubsection{R}
The library \textit{spe} implements the stochastic proximity embedding algorithm.

<<echo=TRUE>>=
library(spe)
res.spe <- spe(data.omit[, -c(1:3)], edim = 2)
Y.spe <- cbind(data.omit[, c("Country", "Year")], res.spe$x)
colnames(Y.spe)[c(3, 4)] <- c("Comp1", "Comp2")
q.spe <- myplot(data = Y.spe)
@

<<echo=FALSE, results = hide>>=
pdf("../../documents/graphs/spe.pdf", height = 8, width = 17)
print(q.spe)
dev.off()
@

\begin{figure}[!htbp]
\begin{center}
\includegraphics{graphs/spe.pdf}
\caption{Stochastic Proximity Embedding.}
\end{center}
\end{figure}

\newpage
\section{Local nonlinear techniques}
The previous section presented some techniques for dimensionality reduction that attempt to retain global properties of
the data. In contrast, local nonlinear techniques for dimensionality reduction are based on solely preserving properties
of small neighborhoods around the datapoints. The key idea behind these techniques is that by preservation of local
properties of the data, the global layout of the data manifold is retained as well.

\subsection{Local Linear Embedding}
\subsubsection{Method}
Local linear embedding (LLE) is another approach which address the problem of non-linear dimensionality reduction
by computing low-dimensional, neighborhood preserving embedding of
high-dimensional data. A data set of dimensionality $D$, which is
assumed to lie on or near a smooth nonlinear manifold of
dimensionality $d < D$, is mapped into a single global coordinate system of lower dimensionality, $d$. The global nonlinear structure is recovered by locally
linear fits.

Consider $n$ $D$-dimensional real-valued vectors $x_i$ sampled from some underlying manifold. We can assume each
data point and its neighbors lie on, or are close to, a locally linear patch of the manifold. By a linear mapping,
consisting of a translation, rotation, and rescaling, the high-dimensional coordinates of each neighbourhood can be
mapped to global internal coordinates on the manifold. Thus, the nonlinear structure of the data can be identified
through two linear steps: first, compute the locally linear patches, and second, compute the linear mapping to the
coordinate system on the manifold.

The main goal here is to map the high-dimensional data points to the single global coordinate system of the
manifold such that the relationships between neighboring points are preserved. This proceeds in three steps:

\begin{enumerate}
  \item Identify the neighbors of each data point $x_i$. This can be done by finding the $k$ nearest neighbors, or by choosing
all points within some fixed radius.
  \item Compute the weights that best linearly reconstruct $x_i$ from its neighbors.
  \item Find the low-dimensional embedding vector $y_i$ which is best reconstructed by the weights determined in the
previous step.
\end{enumerate}

After finding the nearest neighbors in the first step, the second step must compute a local geometry for each locally
linear patch. This geometry is characterized by linear coefficients that reconstruct each data point from its neighbors.

\begin{align*}
 \min_w \sum_{i=1}^n ||x_i - \sum_{j=1}^k w_{ij}x_{N_i(j)}||^2,
\end{align*}
where $N_i(j)$ is the index of the $j$th neighbor of the $i$th point. It then selects code vectors so as to preserve the
reconstruction weights by solving

\begin{align*}
 \min_Y \sum_{i=1}^n ||y_i - \sum_{j=1}^k w_{ij}y_{N_i(j)}||^2
\end{align*}
It can be shown that the coordinates of the low-dimensional representations $y_i$ that minimize this cost function can be
found by computing the eigenvectors corresponding to the smallest d
nonzero eigenvalues of the inproduct of $(I - W)$.
In this formula, $I$ is the $n \times n$ identity matrix and $W$ - reconstruction weights.

\textbf{Strong points}
\begin{itemize}
  \item It attempts to preserve solely local properties of the data. As a result, LLE is not so sensitive to short-circuiting,
because only a small number of properties are affected if short-circuiting occurs.
\end{itemize}

\textbf{Weak points}
\begin{itemize}
  \item LLE tends to collapse large portions of the data onto a single point in cases where the target dimensionality is
too low.
\end{itemize}

\subsubsection{R}
The package \textit{lle} provides some functions which implements a non-linear algorithm for mapping high-dimensional data
into a lower dimensional space.

<<echo=TRUE>>=
library(lle)
k.info <- calc_k(data.omit[, -c(1:3)], m = 2, plotres = FALSE)
k.opt <- k.info[which.min(k.info$rho), "k"]
res.lle <- lle(data.omit[, -c(1:3)], m = 2, k = k.opt)
Y.lle <- cbind(data.omit[, c("Country", "Year")], res.lle$Y)
colnames(Y.lle)[c(3, 4)] <- c("Comp1", "Comp2")
q.lle <- myplot(data = Y.lle)
@

<<echo=FALSE, results = hide>>=
pdf("../../documents/graphs/lle.pdf", height = 8, width = 17)
print(q.lle)
dev.off()
@

\begin{figure}[!htbp]
\begin{center}
\includegraphics{graphs/lle.pdf}
\caption{Local Linear Embedding.}
\end{center}
\end{figure}

The function \textit{calc\_k()} implements the procedure for finding the optimal number of neighbors that is needed to be
provided as the input for the \textit{lle()} function.

<<echo=FALSE>>=
setwd("../../documents")
@

\end{document}

